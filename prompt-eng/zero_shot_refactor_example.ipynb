{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting refers to a technique in prompt engineering where you provide a model with a task without any prior examples. The model is expected to understand and generate a response or complete the task purely based on the given instruction.\n",
    "\n",
    "In other words, the model is given \"zero\" prior training examples or demonstrations in the prompt and relies on its pre-trained knowledge to infer what is needed.\n",
    "\n",
    "## References:\n",
    "* [Wei et al. (2022)](https://arxiv.org/pdf/2109.01652.pdf): demonstrate how instruction tuning improves zero-shot learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fzero_shot.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To calculate `984 * log(2)`, we need to know what base the logarithm is in. Since you didn't specify a base, I will assume that you want the logarithm in base 10.\n",
      "\n",
      "The formula for the logarithm of a number `x` in base `b` is:\n",
      "\n",
      "log_b(x) = log(x / b^n)\n",
      "\n",
      "where `n` is the power to which the number `b` must be raised to produce `x`.\n",
      "\n",
      "Since you want the logarithm in base 10, we can substitute `b = 10`:\n",
      "\n",
      "log(984) = log(984 / 10^n)\n",
      "\n",
      "Now, we need to find the value of `n` that makes the result equal to 4. Since:\n",
      "\n",
      "log(984) = 4\n",
      "\n",
      "We can solve for `n`:\n",
      "\n",
      "n = log(984) / log(10)\n",
      "= 4 / 2 = 2\n",
      "\n",
      "Therefore, the value of `n` that makes the result equal to 4 is 2.\n",
      "Time taken: 1887ms\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## ZERO SHOT PROMPTING\n",
    "##\n",
    "\n",
    "from config import config_factory\n",
    "from clients import ChatbotClientFactory\n",
    "\n",
    "CONFIG = config_factory() \n",
    "\n",
    "#### (1) Adjust the inbounding  Prompt, simulating inbounding requests from users or other systems\n",
    "message = \"What is 984 * log(2)\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "prompt = message\n",
    "\n",
    "#### (3) Instantiate ChatBot Client\n",
    "client = ChatbotClientFactory.create_client(CONFIG)\n",
    "\n",
    "#### (4) Pick a model from the available models\n",
    "models = client.get_models()\n",
    "chosen_model = models[0]\n",
    "\n",
    "for model in models:\n",
    "    if model.name == \"llama2:latest\":\n",
    "        chosen_model = model\n",
    "\n",
    "#### (5) Send the prompt to the model and get the response\n",
    "time, response = client.chat(message=prompt,\n",
    "                             model=chosen_model,\n",
    "                             options={\n",
    "                                 \"max_tokens\": 1000,\n",
    "                                 \"temperature\": 1.0,\n",
    "                             }\n",
    ")\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Models:\n",
      "model_name: llava:latest, model_parameters: 7B\n",
      "model_name: codestral:latest, model_parameters: 22.2B\n",
      "model_name: llama2:latest, model_parameters: 7B\n",
      "model_name: llama3.2-vision:latest, model_parameters: 9.8B\n",
      "model_name: phi4:latest, model_parameters: 14.7B\n",
      "model_name: gemma2:27b, model_parameters: 27.2B\n",
      "model_name: qwen2:latest, model_parameters: 7.6B\n",
      "model_name: mistral:latest, model_parameters: 7.2B\n",
      "model_name: tinyllama:latest, model_parameters: 1B\n",
      "model_name: llama3:latest, model_parameters: 8.0B\n",
      "model_name: koesn/mistral-7b-instruct:latest, model_parameters: 7B\n",
      "model_name: koesn/llama3-8b-instruct:latest, model_parameters: 8B\n",
      "model_name: mistral-large:latest, model_parameters: 122.6B\n"
     ]
    }
   ],
   "source": [
    "## LIST AVAILABLE MODELS\n",
    "\n",
    "## Highly recommend not using models over 7-10B Parameters as the cluster running these seems to time out\n",
    "## On anything larger.\n",
    "\n",
    "print(\"Available Models:\")\n",
    "for model in models:\n",
    "    print(f\"model_name: {model.name}, model_parameters: {model.parameter_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How to improve it?\n",
    "\n",
    "* **Use Clear and Concise Instructions**: Be specific about the task and desired format.\n",
    "    * Bad Prompt: “Summarize this.”\n",
    "    * Good Prompt: “Summarize this paragraph in one sentence.”\n",
    "* **Add Context**: Providing background can help the model interpret ambiguous prompts better.\n",
    "* **Specify Output Format**: If a particular structure is needed, describe it in the instruction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
